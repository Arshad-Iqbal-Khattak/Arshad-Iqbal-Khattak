{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-14T08:54:23.251501Z","iopub.execute_input":"2023-06-14T08:54:23.251857Z","iopub.status.idle":"2023-06-14T08:54:23.288766Z","shell.execute_reply.started":"2023-06-14T08:54:23.251829Z","shell.execute_reply":"2023-06-14T08:54:23.287843Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/chohen7000/Chohen7000.csv\n/kaggle/input/cohen5000/chohen5000.csv\n/kaggle/input/chone3000/chone300.csv\n/kaggle/input/chonen100new/chone1000new.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nimport requests\n\n# API endpoint\napi_endpoint = 'https://api.semanticscholar.org/v1/paper/'\n\n# Function to fetch paper details\ndef fetch_paper_details(paper_id):\n    response = requests.get(api_endpoint + paper_id)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return None\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:54:25.471231Z","iopub.execute_input":"2023-06-14T08:54:25.471599Z","iopub.status.idle":"2023-06-14T08:54:25.477360Z","shell.execute_reply.started":"2023-06-14T08:54:25.471571Z","shell.execute_reply":"2023-06-14T08:54:25.476097Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def update_dataset_with_paper_details(dataset):\n    updated_dataset = []\n    for row in dataset:\n        citing_id = row[0]\n        cited_id = row[1]\n        cited_cl=row[2]\n        cited_sec=row[3]\n        \n        citing_details = fetch_paper_details(citing_id)\n        cited_details = fetch_paper_details(cited_id)\n        \n        if citing_details is not None and cited_details is not None:\n            citing_title = citing_details.get('title', '')\n            citing_abstract = citing_details.get('abstract', '')\n            cited_title = cited_details.get('title', '')\n            cited_abstract = cited_details.get('abstract', '')\n            \n            updated_row = row + [citing_title, citing_abstract, cited_title, cited_abstract]\n            updated_dataset.append(updated_row)\n    \n    return updated_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:54:39.887521Z","iopub.execute_input":"2023-06-14T08:54:39.887877Z","iopub.status.idle":"2023-06-14T08:54:39.895083Z","shell.execute_reply.started":"2023-06-14T08:54:39.887847Z","shell.execute_reply":"2023-06-14T08:54:39.893747Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import csv\n\ndef load_dataset_from_csv(csv_file):\n    dataset = []\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n        for row in reader:\n            dataset.append(row)\n    return dataset\n\n# Main code\nif __name__ == '__main__':\n    # Define the CSV file path\n    csv_file = '/kaggle/input/chohen7000/Chohen7000.csv'\n    \n    dataset = load_dataset_from_csv(csv_file)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:54:35.211871Z","iopub.execute_input":"2023-06-14T08:54:35.212519Z","iopub.status.idle":"2023-06-14T08:54:35.222117Z","shell.execute_reply.started":"2023-06-14T08:54:35.212489Z","shell.execute_reply":"2023-06-14T08:54:35.221333Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":" updated_dataset7000n = update_dataset_with_paper_details(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T10:58:49.870897Z","iopub.execute_input":"2023-06-14T10:58:49.871280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming you have an updated dataset in a variable called updated_dataset\n\n# Convert the updated dataset to a DataFrame\ndf = pd.DataFrame(updated_dataset7000n)\n\n# Save the DataFrame to an Excel file\ndf.to_excel('updated_dataset7000n.xlsx', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T10:58:46.819105Z","iopub.status.idle":"2023-06-14T10:58:46.819431Z","shell.execute_reply.started":"2023-06-14T10:58:46.819280Z","shell.execute_reply":"2023-06-14T10:58:46.819295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Concating Both the file****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the first Excel dataset\ndf1 = pd.read_excel('/kaggle/working/updated_dataset1000n.xlsx')\n\n# Load the second Excel dataset\ndf2 = pd.read_excel('/kaggle/working/updated_dataset300.xlsx')\n\nconcatenated_df = pd.concat([df1, df2])\n\n# Save the concatenated DataFrame to a new Excel file\nconcatenated_df.to_excel('concatenated_dataset.xlsx', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:16:56.094413Z","iopub.execute_input":"2023-06-12T13:16:56.094792Z","iopub.status.idle":"2023-06-12T13:16:58.012040Z","shell.execute_reply.started":"2023-06-12T13:16:56.094764Z","shell.execute_reply":"2023-06-12T13:16:58.010798Z"},"trusted":true},"execution_count":21,"outputs":[]}]}